{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arseniy-Polyakov/speech_technologies_course/blob/main/Russian_TTS_with_IPA_G2P_FastPitch_and_HifiGAN_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_xtPhVAdKhP"
      },
      "source": [
        "Install NeMo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKqBMpNuEMVL"
      },
      "outputs": [],
      "source": [
        "# Install NeMo library. If you are running locally (rather than on Google Colab), comment out the below lines\n",
        "# and instead follow the instructions at https://github.com/NVIDIA/NeMo#Installation\n",
        "GITHUB_ACCOUNT = \"NVIDIA\"\n",
        "BRANCH = \"main\"\n",
        "!python -m pip install git+https://github.com/{GITHUB_ACCOUNT}/NeMo.git@{BRANCH}#egg=nemo_toolkit[all]\n",
        "\n",
        "# Download local version of NeMo scripts. If you are running locally and want to use your own local NeMo code,\n",
        "# comment out the below lines and set NEMO_DIR to your local path.\n",
        "NEMO_DIR = 'nemo'\n",
        "!git clone -b {BRANCH} https://github.com/{GITHUB_ACCOUNT}/NeMo.git $NEMO_DIR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y nemo-toolkit\n",
        "!pip install nemo-toolkit[all]"
      ],
      "metadata": {
        "id": "MrQtWPo9Itco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XluZUtgV09N"
      },
      "source": [
        "Make imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tHuyhJazErbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70a0b5c-41d0-4774-d64a-5c2cbbfc9ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-10-15 13:54:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "      m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "    \n",
            "[NeMo W 2025-10-15 13:54:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "      m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "    \n",
            "[NeMo W 2025-10-15 13:54:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "      elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "    \n",
            "[NeMo W 2025-10-15 13:54:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "      elif re.match('(dbl)p?( \\(default\\))?$', token):\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import IPython.display as ipd\n",
        "import re\n",
        "import soundfile as sf\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from nemo.collections.tts.models import FastPitchModel\n",
        "from nemo.collections.tts.models import HifiGanModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9MMFx0_V3gW"
      },
      "source": [
        "Define file names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zfq3upYnJcpE"
      },
      "outputs": [],
      "source": [
        "INPUT_TEXT = \"input_text.txt\"\n",
        "INPUT_FOR_G2P = \"input_for_g2p.txt\"\n",
        "OUTPUT_OF_G2P = \"output_of_g2p.txt\"\n",
        "INPUT_TEXT_PHONEMES = \"input_text_phonemes.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB0MFkCmWBmn"
      },
      "source": [
        "Create file with some input text.\n",
        "Note that text normalization (conversion of digits to words etc.) is **not** included in this pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2jQF1F3OFYri"
      },
      "outputs": [],
      "source": [
        "!echo \"(Я представляю себе вашу ироническую улыбку. Тем не менее – буквально два слова.) Как известно, мир несовершенен.\" > {INPUT_TEXT}\n",
        "!echo \"Устоями общества являются корыстолюбие, страх и продажность.\" >> {INPUT_TEXT}\n",
        "!echo \"Конфликт мечты с действительностью не утихает тысячелетиями.\" >> {INPUT_TEXT}\n",
        "!echo \"Вместо желаемой гармонии на земле царят хаос и беспорядок.\" >> {INPUT_TEXT}\n",
        "!echo \"Более того, нечто подобное мы обнаружили в собственной душе.\" >> {INPUT_TEXT}\n",
        "!echo \"Мы жаждем совершенства, а вокруг торжествует пошлость. Как в этой ситуации поступает деятель, революционер?\" >> {INPUT_TEXT}\n",
        "!echo \"Революционер делает попытки установить мировую гармонию.\" >> {INPUT_TEXT}\n",
        "!echo \"Он начинает преобразовывать жизнь, достигая иногда курьезных мичуринских результатов.\" >> {INPUT_TEXT}\n",
        "!echo \"Допустим, выводит морковь, совершенно неотличимую от картофеля. В общем, создает новую человеческую породу.\" >> {INPUT_TEXT}\n",
        "!echo \"Известно, чем это кончается… Что в этой ситуации предпринимает моралист? Он тоже пытается достичь гармонии.\" >> {INPUT_TEXT}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imZla8ZpWzmZ"
      },
      "source": [
        "Some helper preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A3nGXsYTI6GW"
      },
      "outputs": [],
      "source": [
        "def clean_russian_g2p_trascription(text: str) -> str:\n",
        "    result = text\n",
        "    result = result.replace(\"<DELETE>\", \" \").replace(\"+\", \"\").replace(\"~\", \"\")\n",
        "    result = result.replace(\"ʑ\", \"ɕ:\").replace(\"ɣ\", \"x\")\n",
        "    result = result.replace(\":\", \"ː\").replace(\"'\", \"`\")\n",
        "    result = \"\".join(result.split())\n",
        "    result = result.replace(\"_\", \" \")\n",
        "    return result\n",
        "\n",
        "\n",
        "def clean_russian_text_for_tts(text: str) -> str:\n",
        "    result = text\n",
        "    result = result.replace(\"+\", \"\")  # remove stress\n",
        "    result = result.casefold()  # lowercase\n",
        "    result = result.replace(\"ё\", \"е\")\n",
        "    result = result.replace(\"\\u2011\", \"-\")  # non-breaking hyphen\n",
        "    result = result.replace(\"\\u2014\", \"-\")  # em dash\n",
        "    result = result.replace(\"\\u2026\", \".\")  # horizontal ellipsis\n",
        "    result = result.replace(\"\\u00ab\", \"\\\"\")  # LEFT-POINTING DOUBLE ANGLE QUOTATION MARK\n",
        "    result = result.replace(\"\\u00bb\", \"\\\"\")  # RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK\n",
        "    result = result.replace(\"\\u2019\", \"'\")  # ’ Right Single Quotation Mark\n",
        "    result = result.replace(\"\\u201c\", \"\\\"\")  # “ Left Double Quotation Mark\n",
        "    result = result.replace(\"\\u201d\", \"\\\"\")  # ” Right Double Quotation Mark\n",
        "    result = result.replace(\"\\u201e\", \"\\\"\")  # „ Double Low-9 Quotation Mark\n",
        "    result = result.replace(\"\\u201f\", \"\\\"\")  # ‟ Double High-reversed-9 Quotation Mark\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju6jWFQUW8Ae"
      },
      "source": [
        "Take all unique words from the input text and prepare them to feed to G2P model.\n",
        "Note that G2P model works with separate words and does not take context into account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7tOckIa1JNPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eec7def-eef6-44c4-ce43-67a5a8c32ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-10-15 14:00:04 nemo_logging:405] <>:5: SyntaxWarning: invalid escape sequence '\\w'\n",
            "    \n",
            "[NeMo W 2025-10-15 14:00:04 nemo_logging:405] <>:5: SyntaxWarning: invalid escape sequence '\\w'\n",
            "    \n",
            "[NeMo W 2025-10-15 14:00:04 nemo_logging:405] /tmp/ipython-input-670115531.py:5: SyntaxWarning: invalid escape sequence '\\w'\n",
            "      words = re.compile('\\w+').findall(text)\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "all_words = set()\n",
        "with open(INPUT_TEXT, \"r\", encoding=\"utf-8\") as inp:\n",
        "    for line in inp:\n",
        "        text = line.strip()\n",
        "        words = re.compile('\\w+').findall(text)\n",
        "        for w in words:\n",
        "            all_words.add(clean_russian_text_for_tts(w))\n",
        "\n",
        "with open(INPUT_FOR_G2P, \"w\", encoding=\"utf-8\") as out:\n",
        "    for w in all_words:\n",
        "        out.write(\" \".join(list(w)) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kO6QZmCbKRd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87068f63-04b8-47b4-8af0-282182f92e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "с т р а х\n",
            "в\n",
            "у т и х а е т\n",
            "б е с п о р я д о к\n",
            "в о к р у г\n",
            "п р е о б р а з о в ы в а т ь\n",
            "у с т о я м и\n",
            "н е ч т о\n",
            "и\n",
            "и з в е с т н о\n"
          ]
        }
      ],
      "source": [
        "!head {INPUT_FOR_G2P}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vM1tso1XnvF"
      },
      "source": [
        "Clone [G2P model](https://huggingface.co/bene-ges/ru_g2p_ipa_bert_large) from HuggingFace.\n",
        "If cloning doesn't work try `git lfs install`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YjgXGRtzMqpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ac7a21-79a9-4ea8-f627-2006f5696358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ru_g2p_ipa_bert_large' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/bene-ges/ru_g2p_ipa_bert_large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtiNwKhOYKrP"
      },
      "source": [
        "Run G2P inference on the words that we prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "00OyJiTKMX23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdb9759-c97f-4ba6-b751-a3b566dc47b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/{NEMO_DIR}/examples/nlp/text_normalization_as_tagging/normalization_as_tagging_infer.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python {NEMO_DIR}/examples/nlp/text_normalization_as_tagging/normalization_as_tagging_infer.py \\\n",
        "  pretrained_model=ru_g2p_ipa_bert_large/ru_g2p.nemo \\\n",
        "  inference.from_file={INPUT_FOR_G2P} \\\n",
        "  inference.out_file={OUTPUT_OF_G2P} \\\n",
        "  model.max_sequence_len=512 \\\n",
        "  inference.batch_size=128 \\\n",
        "  lang=ru\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uu9rgtKcP2H-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84a2f50-0df0-4076-c99a-1c9ec3ff6dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head: cannot open 'output_of_g2p.txt' for reading: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!head {OUTPUT_OF_G2P}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khu8G5qiQG4e"
      },
      "source": [
        "Preprocess input text for TTS using G2P results and vocabularies of known transcriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "z1M5lytAQFlu"
      },
      "outputs": [],
      "source": [
        "# heteronyms are words with ambiguous transcription, we will leave them as plain text\n",
        "heteronyms = set()\n",
        "with open(\"ru_g2p_ipa_bert_large/heteronyms.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        inp = line.strip()\n",
        "        heteronyms.add(inp)\n",
        "\n",
        "g2p_vocab = {}\n",
        "# first read transcriptions from our g2p prediction\n",
        "with open(OUTPUT_OF_G2P, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            _, inp, transcription, _, _ = line.strip().split(\"\\t\")\n",
        "        except:\n",
        "            print(\"cannot read line: \" + line)\n",
        "            continue\n",
        "        inp = inp.replace(\" \", \"\")\n",
        "        g2p_vocab[inp] = clean_russian_g2p_trascription(transcription)\n",
        "\n",
        "# then override known transcriptions using vocabulary\n",
        "with open(\"ru_g2p_ipa_bert_large/g2p_correct_vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Example input: ледок \\t lʲɪd`ok\n",
        "        inp, transcription = line.strip().split(\"\\t\")\n",
        "        g2p_vocab[inp] = transcription\n",
        "\n",
        "out = open(INPUT_TEXT_PHONEMES, \"w\", encoding=\"utf-8\")\n",
        "\n",
        "with open(INPUT_TEXT, \"r\", encoding=\"utf-8\") as inp:\n",
        "    for line in inp:\n",
        "        text = line.strip()\n",
        "        text = clean_russian_text_for_tts(text)\n",
        "        phonemized_text = \"\"\n",
        "        m = re.search(r\"[\\w\\-]+\", text)\n",
        "        while m is not None:\n",
        "            begin = m.start()\n",
        "            end = m.end()\n",
        "            phonemized_text += text[0:begin]\n",
        "            w = text[begin:end]\n",
        "            if w in heteronyms:\n",
        "                phonemized_text += w\n",
        "            elif w in g2p_vocab:\n",
        "                phonemized_text += clean_russian_g2p_trascription(g2p_vocab[w])\n",
        "            else:  # shouldn't go here as all words are expected to pass through g2p\n",
        "                phonemized_text += w\n",
        "\n",
        "            if end >= len(text):\n",
        "                break\n",
        "            text = text[end:]\n",
        "            end = 0\n",
        "            m = re.search(r\"[\\w\\-]+\", text)\n",
        "        if end < len(text):\n",
        "            phonemized_text += text[end:]\n",
        "\n",
        "        out.write(phonemized_text + \"\\n\")\n",
        "\n",
        "out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfqPBe1oYsX_"
      },
      "source": [
        "Look at the final TTS input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6wpxmChtTFnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34d2a90-960e-4e46-b843-6179f4330180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(ja prʲɪtstɐvlʲ`æjʊ sʲɪbʲ`e v`aʂʊ ɪrɐnʲ`itɕɪskʊjʊ ʊɫ`ɨpkʊ. тем не mʲ`enʲɪje – bʊkv`alʲnə dva слова.) kak ɪzvʲ`esnə, mʲir nʲɪsəvʲɪrʂ`ɛnʲɪn.\n",
            "ʊst`ojəmʲɪ `opɕːɪstvə jɪvlʲ`æjʊtsə корыстолюбие, strax i prɐd`aʐnəsʲtʲ.\n",
            "kɐnflʲ`ikt mʲɪtɕt`ɨ s dʲɪjstvʲ`itʲɪlʲnəsʲtʲjʊ не ʊtʲɪx`ajɪt tɨsʲɪtɕɪlʲ`etʲɪjəmʲɪ.\n",
            "vmʲ`estə ʐɨɫ`ajɪməj ɡɐrm`onʲɪɪ на zʲɪmlʲ`e tsɐrʲ`at хаос i bʲɪspɐrʲ`adək.\n",
            "b`olʲɪje того, nʲ`eʂtə pɐd`obnəjə mɨ ɐbnɐr`uʐɨlʲɪ v s`opstvʲɪnːəj душе.\n",
            "mɨ ʐ`aʐdʲɪm səvʲɪrʂ`ɛnstvə, a vɐkr`uk tərʐɨstv`ujɪt p`oʂɫəsʲtʲ. kak v `ɛtəj sʲɪtʊ`atsɨɪ pəstʊp`ajɪt dʲ`ejɪtʲɪlʲ, rʲɪvəlʲʊtsɨɐnʲ`er?\n",
            "rʲɪvəlʲʊtsɨɐnʲ`er dʲ`eɫəjɪt pɐp`ɨtkʲɪ ʊstənɐvʲ`itʲ mʲɪrɐv`ujʊ ɡɐrm`onʲɪjʊ.\n",
            "on nətɕɪn`ajɪt prʲɪəbrɐz`ovɨvətʲ ʐɨzʲnʲ, dəsʲtʲɪɡ`ajə ɪnɐɡd`a kʊrʲ`jɵznɨx мичуринских rʲɪzʊlʲt`atəf.\n",
            "допустим, vɨv`odʲɪt mɐrk`ofʲ, səvʲɪrʂ`ɛnːə nʲɪətlʲɪtɕ`imʊjʊ от kɐrt`ofʲɪlʲə. v `opɕːɪm, səzdɐ`jɵt n`ovʊjʊ tɕɪɫɐvʲ`etɕɪskʊjʊ pɐr`odʊ.\n",
            "ɪzvʲ`esnə, чем `ɛtə kɐnʲtɕ`æjɪtsə. ʂto v `ɛtəj sʲɪtʊ`atsɨɪ prʲɪtprʲɪnʲɪm`ajɪt mərɐlʲ`ist? on t`oʐɨ pɨt`ajɪtsə dɐsʲtʲ`itɕ ɡɐrm`onʲɪɪ.\n"
          ]
        }
      ],
      "source": [
        "!head {INPUT_TEXT_PHONEMES}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF2PrP7VZKb-"
      },
      "source": [
        "Run TTS. The resulting wav files will be saved to working directory and also displayed in the output cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMB7wqKOFLbP"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "\n",
        "# Load FastPitch\n",
        "spectrogram_generator = FastPitchModel.from_pretrained(\"bene-ges/tts_ru_ipa_fastpitch_ruslan\").eval().to(device)\n",
        "# Load vocoder\n",
        "vocoder = HifiGanModel.from_pretrained(model_name=\"bene-ges/tts_ru_hifigan_ruslan\").eval().to(device)\n",
        "\n",
        "i = 0\n",
        "with open(INPUT_TEXT_PHONEMES, \"r\", encoding=\"utf-8\") as inp:\n",
        "    for line in inp:\n",
        "        text = line.strip()\n",
        "        parsed = spectrogram_generator.parse(text)\n",
        "        spectrogram = spectrogram_generator.generate_spectrogram(tokens=parsed)\n",
        "        audio = vocoder.convert_spectrogram_to_audio(spec=spectrogram)\n",
        "\n",
        "        # Note that vocoder return a batch of audio. In this example, we just take the first and only sample.\n",
        "        filename = str(i) + \".wav\"\n",
        "        sf.write(filename, audio.to('cpu').detach().numpy()[0], 22050)\n",
        "        i += 1\n",
        "\n",
        "        # display\n",
        "        print(f'\"{text}\"\\n')\n",
        "        ipd.display(ipd.Audio(audio.to('cpu').detach(), rate=22050))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}